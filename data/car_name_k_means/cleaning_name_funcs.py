from hazm import word_tokenize


# excess and unuseful data

adjectives = ['آماده‌',
              'تمیز', 'سالم', 'نو', 'کم کار', 'کمکار', 'بی رنگ', 'بی‌رنگ', 'بینظیر', 'فول', 'فول آپشن',
              'فول‌اپشن', 'فولاپشن', 'لاکچری', 'زیره پا نخورده', 'عروس', 'فابریک', 'کارکرده', 'بی‌خط', 'بی خط',
              'در', 'حد', 'صفر', 'درحدصفر', 'درحد', 'بینقص', 'سفارشی', 'دوگانه', 'تک گانه', 'دورنگ', 'بیمه دار',
              'آماده تحویل', 'تمیز و مرتب', 'فنی سالم', 'خوش رکاب', 'بدون', 'رنگ', 'بدون خط و خش', 'آماده', 'سفر', 'تک‌', 'برگ', 'مدل', 'دوگانه سوز', 'سوز', '،', 'حواله', 'بی',
              'خرج', 'سیلندر', 'توربو', 'شارژ', 'خانگی', 'تازه', 'شده', 'کامل', 'موتور', 'جدید', 'استثنایی', 'ارتقا', 'یافته', 'فلز', 'اپشنال', 'آپشنال', 'شرط', 'به', 'اسفند', 'سفارشی'
              ]
docs_words = [
    'تک برگ', 'سند تک برگ', 'سند', 'کارکرد', 'بیمه', 'برگه', 'کارشناسی', 'برگه', 'قولنامه', 'مالک',
    'مدارک کامل', 'برگه کمپانی', 'تخفیف بیمه', 'سال تخفیف', 'بدون خلافی', 'پلاک ملی', 'بنزینی',
    'دوگانه سوز', 'دوگانه‌سوز', 'دوگانه', 'حواله', 'مدرک', 'مدارک', 'جزییات'
]
colors = [
    'سفید', 'مشکی', 'طوسی', 'نقره‌ای', 'خاکستری', 'آبی', 'قرمز', 'زرشکی', 'سبز', 'بژ',
    'زرد', 'نارنجی', 'قهوه‌ای', 'طلایی', 'موکا', 'یخی', 'نوک‌مدادی', 'دلفینی',
    'white', 'black', 'gray', 'grey', 'blue', 'red', 'green', 'silver', 'gold', 'orange', 'brown'
]
years = [y for y in range(1370, 1406)] + [y for y in range(1990, 2026)] + \
    [y for y in range(50, 100)] + [y for y in range(400, 404)]
unuseful_words = adjectives + docs_words + colors + years


def filter_unuseful_words(name):
    token = word_tokenize(name)
    for word in token.copy():
        try:
            int_word = int(word)
            if int_word in unuseful_words:
                token.remove(word)

        except:
            if word in unuseful_words:
                token.remove(word)

    return " ".join(token)


# print(filter_unuseful_words("سمند سورن پلاس، مدل ۱۴۰۳"))
